name: Terraform Destroy
on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to destroy'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - staging
          - prod
      confirmation:
        description: 'Type "DESTROY" to confirm destruction'
        required: true
        type: string

jobs:
  terraform-destroy:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.confirmation == 'DESTROY' }}
    environment: ${{ github.event.inputs.environment }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3
        
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'us-west-2' }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7
          
      - name: Fetch Terraform Backend Details
        id: backend_details
        run: |
          ENV="${{ github.event.inputs.environment }}"
          
          echo "Attempting to retrieve backend details from SSM..."
          
          # Try to get the bucket name from SSM, but use a default if not found
          if ! TF_STATE_BUCKET=$(aws ssm get-parameter --name "/terraform/$ENV/state_bucket" --query "Parameter.Value" --output text 2>/dev/null); then
            echo "SSM parameter for state bucket not found, using default naming convention"
            TF_STATE_BUCKET="neo4j-demos-terraform-state-$ENV"
          fi
          
          # Try to get the lock table name from SSM, but use a default if not found
          if ! TF_LOCK_TABLE=$(aws ssm get-parameter --name "/terraform/$ENV/lock_table" --query "Parameter.Value" --output text 2>/dev/null); then
            echo "SSM parameter for lock table not found, using default naming convention"
            TF_LOCK_TABLE="neo4j-demos-terraform-locks-$ENV"
          fi
          
          echo "Using backend details:"
          echo "  - Bucket: $TF_STATE_BUCKET"
          echo "  - Table: $TF_LOCK_TABLE"
          
          # Set output variables for use in other steps
          echo "bucket_name=$TF_STATE_BUCKET" >> $GITHUB_OUTPUT
          echo "table_name=$TF_LOCK_TABLE" >> $GITHUB_OUTPUT
          
          # Also set environment variables for this job
          echo "TF_STATE_BUCKET=$TF_STATE_BUCKET" >> $GITHUB_ENV
          echo "TF_LOCK_TABLE=$TF_LOCK_TABLE" >> $GITHUB_ENV
      
      - name: Get EKS Cluster Name
        id: eks_cluster
        run: |
          ENV="${{ github.event.inputs.environment }}"
          CLUSTER_NAME=$(aws ssm get-parameter --name "/neo4j-demos/$ENV/eks-cluster-name" --query "Parameter.Value" --output text 2>/dev/null || echo "neo4j-demo-cluster")
          echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT
          echo "Using EKS cluster: $CLUSTER_NAME"
      
      - name: Clean up Kubernetes Namespaces
        run: |
          ENV="${{ github.event.inputs.environment }}"
          CLUSTER_NAME="${{ steps.eks_cluster.outputs.cluster_name }}"
          
          echo "Updating kubeconfig for cluster $CLUSTER_NAME..."
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "${{ secrets.AWS_REGION || 'us-west-2' }}" || true
          
          if kubectl get namespaces 2>/dev/null; then
            echo "Cleaning up Kubernetes resources..."
            
            # Get all namespaces related to demos
            DEMO_NAMESPACES=$(kubectl get namespaces -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\n' | grep -E '^neo4j-|^movie-|^social-|^basic-' || echo "")
            
            for NS in $DEMO_NAMESPACES; do
              echo "Deleting namespace: $NS"
              kubectl delete namespace $NS --wait=false
            done
            
            # Delete any ingresses in any namespace that might be using ALB resources
            echo "Checking for ingresses in all namespaces..."
            kubectl get ingress --all-namespaces -o jsonpath='{range .items[*]}{.metadata.namespace}{" "}{.metadata.name}{"\n"}{end}' 2>/dev/null | \
            while read NS NAME; do
              echo "Deleting ingress $NAME in namespace $NS"
              kubectl delete ingress -n $NS $NAME --wait=false
            done
            
            echo "Waiting for resources to start cleaning up..."
            sleep 30
          else
            echo "Could not access Kubernetes API, cluster may not exist or credentials are invalid."
          fi
      
      - name: Clean Up ECR Repositories
        run: |
          echo "Cleaning up ECR repositories before Terraform destroy..."
          
          # List of repositories to clean
          REPOS=("neo4j-basic-demo" "neo4j-social-network" "neo4j-movie-recommendation")
          
          for REPO in "${REPOS[@]}"; do
            echo "Checking repository: $REPO"
            
            # Check if repository exists
            if aws ecr describe-repositories --repository-names "$REPO" 2>/dev/null; then
              echo "Repository $REPO exists, deleting all images..."
              
              # Get image IDs and delete them
              IMAGE_IDS=$(aws ecr list-images --repository-name "$REPO" --query 'imageIds[*]' --output json)
              
              if [ "$IMAGE_IDS" != "[]" ] && [ -n "$IMAGE_IDS" ]; then
                echo "Deleting images from $REPO..."
                aws ecr batch-delete-image --repository-name "$REPO" --image-ids "$IMAGE_IDS" || true
                echo "Waiting for image deletion to complete..."
                sleep 10
              else
                echo "No images found in $REPO"
              fi
            else
              echo "Repository $REPO not found"
            fi
          done
      
      - name: Clean Up EKS Resources
        run: |
          echo "Cleaning up EKS resources before Terraform destroy..."
          CLUSTER_NAME="${{ steps.eks_cluster.outputs.cluster_name }}"
          
          # Check if EKS cluster exists
          if aws eks describe-cluster --name "$CLUSTER_NAME" 2>/dev/null; then
            echo "EKS cluster exists, cleaning up dependent resources..."
            
            # Get all node groups and delete them first
            NODE_GROUPS=$(aws eks list-nodegroups --cluster-name "$CLUSTER_NAME" --query "nodegroups[]" --output text 2>/dev/null || echo "")
            
            for NG in $NODE_GROUPS; do
              echo "Deleting node group: $NG"
              aws eks delete-nodegroup --cluster-name "$CLUSTER_NAME" --nodegroup-name "$NG"
              echo "Waiting for node group deletion..."
              aws eks wait nodegroup-deleted --cluster-name "$CLUSTER_NAME" --nodegroup-name "$NG" || true
            done
            
            # Wait for node groups to be deleted
            if [ -n "$NODE_GROUPS" ]; then
              echo "Waiting for node groups to be deleted..."
              sleep 120
            fi
          else
            echo "EKS cluster not found or already deleted"
          fi
          
      - name: Clean Up Network Resources
        run: |
          echo "Cleaning up network resources before Terraform destroy..."
          
          # Get VPC ID using tags that might match our infrastructure
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=*neo4j-demos*" --query "Vpcs[0].VpcId" --output text)
          
          if [ "$VPC_ID" != "None" ] && [ -n "$VPC_ID" ]; then
            echo "Found VPC: $VPC_ID"
            
            # Release Elastic IPs
            echo "Checking for Elastic IPs..."
            EIP_ALLOCS=$(aws ec2 describe-addresses --filters "Name=domain,Values=vpc" --query "Addresses[?AssociationId].AllocationId" --output text)
            
            for EIP in $EIP_ALLOCS; do
              echo "Releasing Elastic IP: $EIP"
              aws ec2 release-address --allocation-id "$EIP" || true
            done
            
            # Find and delete NAT gateways
            echo "Checking for NAT Gateways..."
            NAT_GWS=$(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_ID" --query "NatGateways[?State!='deleted'].NatGatewayId" --output text)
            
            for NAT in $NAT_GWS; do
              echo "Deleting NAT Gateway: $NAT"
              aws ec2 delete-nat-gateway --nat-gateway-id "$NAT" || true
            done
            
            if [ -n "$NAT_GWS" ]; then
              echo "Waiting for NAT gateways to be deleted..."
              sleep 90
            fi
            
            # Find network interfaces
            echo "Checking for network interfaces..."
            ENIs=$(aws ec2 describe-network-interfaces --filters "Name=vpc-id,Values=$VPC_ID" --query "NetworkInterfaces[].NetworkInterfaceId" --output text)
            
            for ENI in $ENIs; do
              echo "Deleting network interface: $ENI"
              # First try to detach if attached
              ATTACHMENT=$(aws ec2 describe-network-interfaces --network-interface-ids "$ENI" --query "NetworkInterfaces[0].Attachment.AttachmentId" --output text)
              if [ "$ATTACHMENT" != "None" ] && [ -n "$ATTACHMENT" ]; then
                echo "Detaching $ENI first..."
                aws ec2 detach-network-interface --attachment-id "$ATTACHMENT" --force || true
                sleep 5
              fi
              aws ec2 delete-network-interface --network-interface-id "$ENI" || true
            done
          else
            echo "VPC not found or already deleted"
          fi
          
      - name: Verify and Clean ALB Resources
        run: |
          echo "Checking for lingering ALB resources..."
          ENV="${{ github.event.inputs.environment }}"
          
          # List all load balancers and find ones that match your naming pattern
          LBS=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, 'neo4j-demos') || contains(LoadBalancerName, '${ENV}')].LoadBalancerArn" --output text)
          
          if [ -n "$LBS" ]; then
            echo "Found lingering load balancers, attempting cleanup..."
            
            for LB in $LBS; do
              echo "Deleting load balancer: $LB"
              # Delete listeners first to avoid dependency issues
              LISTENERS=$(aws elbv2 describe-listeners --load-balancer-arn $LB --query "Listeners[].ListenerArn" --output text)
              for LISTENER in $LISTENERS; do
                aws elbv2 delete-listener --listener-arn $LISTENER || true
              done
              
              # Delete the load balancer
              aws elbv2 delete-load-balancer --load-balancer-arn $LB || true
            done
            
            echo "Waiting for load balancers to be deleted..."
            sleep 60
          else
            echo "No lingering load balancers found."
          fi
          
          # Check for orphaned target groups
          TGS=$(aws elbv2 describe-target-groups --query "TargetGroups[?contains(TargetGroupName, 'neo4j-demos') || contains(TargetGroupName, '${ENV}')].TargetGroupArn" --output text)
          
          if [ -n "$TGS" ]; then
            echo "Found lingering target groups, attempting cleanup..."
            
            for TG in $TGS; do
              echo "Deleting target group: $TG"
              aws elbv2 delete-target-group --target-group-arn $TG || true
            done
          else
            echo "No lingering target groups found."
          fi

      # First destroy the main infrastructure
      - name: Terraform Init (Main Infrastructure)
        working-directory: Terraform
        continue-on-error: true
        id: terraform_init
        run: |
          terraform init \
            -backend-config="bucket=$TF_STATE_BUCKET" \
            -backend-config="key=terraform.tfstate" \
            -backend-config="dynamodb_table=$TF_LOCK_TABLE" \
            -backend-config="region=${{ secrets.AWS_REGION || 'us-west-2' }}"
          
          # Store the exit code
          INIT_EXIT_CODE=$?
          echo "init_exit_code=$INIT_EXIT_CODE" >> $GITHUB_OUTPUT
          
          if [ $INIT_EXIT_CODE -ne 0 ]; then
            echo "Terraform init failed, attempting alternative approaches..."
          fi
      
      - name: Handle Terraform State Inconsistency
        if: steps.terraform_init.outputs.init_exit_code != '0'
        run: |
          echo "Handling Terraform state inconsistency..."
          
          # Get the current state from S3
          echo "Downloading current state file from S3..."
          aws s3 cp s3://$TF_STATE_BUCKET/terraform.tfstate ./current_state.json || true
          
          # If the state file exists locally, let's try to fix things
          if [ -f "./current_state.json" ]; then
            echo "Found state file, attempting to reset the DynamoDB lock..."
            
            # Calculate MD5 hash of the state file
            STATE_MD5=$(md5sum ./current_state.json | cut -d' ' -f1)
            echo "Calculated MD5: $STATE_MD5"